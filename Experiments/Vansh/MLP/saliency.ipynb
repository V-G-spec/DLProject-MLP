{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "# sys.path.append('../../scaling_mlps/')\n",
    "from models.networks import get_model\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "from data_utils.data_stats import *\n",
    "# from data_utils.dataset_to_beton import get_dataset\n",
    "from utils.metrics import topk_acc, real_acc, AverageMeter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/guptav/DLProject-MLP/Experiments/Vansh/MLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'cifar10'                 # One of cifar10, cifar100, stl10, imagenet or imagenet21\n",
    "architecture = 'B_12-Wi_1024'\n",
    "data_resolution = 32                # Resolution of data as it is stored\n",
    "crop_resolution = 64                # Resolution of fine-tuned model (64 for all models we provide)\n",
    "num_classes = CLASS_DICT[dataset]\n",
    "eval_batch_size = 1024\n",
    "checkpoint = 'in21k_cifar10'        # This means you want the network pre-trained on ImageNet21k and finetuned on CIFAR10\n",
    "data_path = '/home/vgspec/Personal/DLProject-MLP/Data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the model\n",
    "### Define Transformations + Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights already downloaded\n",
      "Load_state output <All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): BottleneckMLP(\n",
       "    (linear_in): Linear(in_features=12288, out_features=1024, bias=True)\n",
       "    (linear_out): Linear(in_features=1024, out_features=10, bias=True)\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x BottleneckBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorms): ModuleList(\n",
       "      (0-11): 12 x LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the model and specify the pre-trained weights\n",
    "model_MLP = get_model(architecture=architecture, resolution=crop_resolution, num_classes=CLASS_DICT[dataset],\n",
    "                  checkpoint='in21k_cifar10')\n",
    "model_MLP = nn.Sequential(\n",
    "    nn.Flatten(1, -1),model_MLP\n",
    ")\n",
    "model_MLP = model_MLP.to(device)\n",
    "model_MLP.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#inverse transform to get normalize image back to original form for visualization\n",
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-0.4914/0.2471, -0.4822/0.2435, -0.4465/0.2616],\n",
    "    std=[1/0.2471, 1/0.2435, 1/0.2616]\n",
    ")\n",
    "\n",
    "#transforms to resize image to the size expected by pretrained model,\n",
    "#convert PIL image to tensor, and\n",
    "#normalize the image\n",
    "transform_c = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2471, 0.2435, 0.2616)),\n",
    "])\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_c)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy on dog images:  1.0\n"
     ]
    }
   ],
   "source": [
    "classes = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "count = 0\n",
    "total = 100\n",
    "for i in range(total):\n",
    "    path_img = f'/home/guptav/DLProject-MLP/Experiments/Vansh/Sample_Images/dog ({i+1}).jpg'\n",
    "    img = resize_image(path_img).convert('RGB')\n",
    "    img_ten = transform_c(img).unsqueeze(0)\n",
    "    img_ten = torch.reshape(img_ten, (img_ten.shape[0], -1))\n",
    "    output = model_MLP(img_ten)\n",
    "    if (np.argmax(output.detach()).item() == 5):\n",
    "        count+=1\n",
    "print(\"Model accuracy on dog images: \", count*100/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ex = 0\n",
    "correct_ex = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        # inputs = torch.reshape(inputs, (inputs.shape[0], -1))\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model_MLP(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_ex += (predicted == labels).sum().item()\n",
    "        total_ex += labels.size(0)\n",
    "        print(\"Correct: \", correct_ex, \"Total: \", total_ex)\n",
    "\n",
    "print(\"Model accuracy on CIFAR10: \", correct_ex*100/total_ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saliency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "#We don't normalize here^ because the input is already normalized\n",
    "def saliency(img, model):\n",
    "    #we don't need gradients w.r.t. weights for a trained model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    #set model in eval mode\n",
    "    model.eval()\n",
    "    #transoform input PIL image to torch.Tensor and normalize\n",
    "    input = deepcopy(img)\n",
    "    input = transform(input)\n",
    "    input.unsqueeze_(0)\n",
    "\n",
    "    #we want to calculate gradient of higest score w.r.t. input\n",
    "    #so set requires_grad to True for input \n",
    "    input.requires_grad = True\n",
    "    #forward pass to calculate predictions\n",
    "    preds = model(input)\n",
    "    score, indices = torch.max(preds, 1)\n",
    "    #backward pass to get gradients of score predicted class w.r.t. input image\n",
    "    score.backward()\n",
    "    #get max along channel axis\n",
    "    slc, _ = torch.max(torch.abs(input.grad[0]), dim=0)\n",
    "    #normalize to [0..1]\n",
    "    slc = (slc - slc.min())/(slc.max()-slc.min())\n",
    "\n",
    "    #apply inverse transform on image\n",
    "    with torch.no_grad():\n",
    "        input_img = inv_normalize(transform(img))\n",
    "    #plot image and its saleincy map\n",
    "    # plt.figure(figsize=(5, 5))\n",
    "    # plt.subplot(1, 2, 1)\n",
    "    # plt.imshow(np.transpose(input_img.detach().numpy(), (1, 2, 0)))\n",
    "    # plt.xticks([])\n",
    "    # plt.yticks([])\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(slc.numpy(), cmap=plt.cm.hot)\n",
    "    # plt.xticks([])\n",
    "    # plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Images Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "for i in range(20):\n",
    "    path_img = f'/home/guptav/DLProject-MLP/Experiments/Vansh/Sample_Images/dog ({i+1}).jpg'\n",
    "    img = resize_image(path_img).convert('RGB')\n",
    "    img_ten = transform_c(img).unsqueeze(0)\n",
    "    img_ten = torch.reshape(img_ten, (img_ten.shape[0], -1))\n",
    "    output = model(img_ten)\n",
    "    if (np.argmax(output.detach()).item() == 5):\n",
    "        saliency(img, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "\n",
    "transform_img = transforms.ToPILImage()\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "    # inputs2 = torch.reshape(inputs, (inputs.shape[0], -1))\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    outputs = model_MLP(inputs)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    for input, label, pred in zip(inputs, labels, predicted):\n",
    "        if (random.random() < 0.01):\n",
    "            if (label == pred):\n",
    "                print(classes[label.item()])\n",
    "                plt.figure(figsize=(10, 10))\n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.imshow(transform_img(inv_normalize(input)))\n",
    "                saliency(transform_img(input), model_MLP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
